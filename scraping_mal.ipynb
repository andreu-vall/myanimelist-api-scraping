{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the MAL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAL API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "api_url = 'https://api.myanimelist.net/v2'\n",
    "\n",
    "# A Client ID is needed (https://myanimelist.net/apiconfig)\n",
    "with open('client_id.txt', 'r') as f:\n",
    "    CLIENT_ID = f.read()\n",
    "\n",
    "headers = {'X-MAL-CLIENT-ID': CLIENT_ID}\n",
    "\n",
    "def get_data(endpoint, params=None):\n",
    "    url = api_url + endpoint\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def scrape_ranking_page(database, ranking_type, page, fields, save_directory, length):\n",
    "    params = {'ranking_type': ranking_type, 'limit': 500, 'offset': page*500, 'fields': fields}\n",
    "    try:\n",
    "        data = get_data(f'/{database}/ranking', params)\n",
    "    except:\n",
    "        data = manga_crash(f'/{database}/ranking', params)\n",
    "    \n",
    "    useful = [anime['node'] for anime in data['data']]\n",
    "    with open(save_directory + f'/page{str(page).zfill(length)}.json', 'w') as f:\n",
    "        json.dump(useful, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "def scrape_ranking(database='anime', ranking_type='favorite'):\n",
    "\n",
    "    base_directory = f'data/raw'\n",
    "    save_file_path = base_directory + f'/{database}_mal.json'\n",
    "    tmp_directory = base_directory + f'/tmp_{database}_mal'\n",
    "    os.makedirs(tmp_directory)\n",
    "\n",
    "    fields = ','.join(keys[database])\n",
    "    last_page = get_last_page(database, ranking_type)\n",
    "    length = len(str(last_page))\n",
    "\n",
    "    start = datetime.datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "    print('Scraped at:', start)\n",
    "    for page in tqdm.trange(last_page+1):\n",
    "        scrape_ranking_page(database, ranking_type, page, fields, tmp_directory, length)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    merge_anime(tmp_directory, save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_last_page(database, ranking_type):\n",
    "\n",
    "    if database=='anime' and ranking_type=='favorite':\n",
    "        number_entries =  24_189\n",
    "    \n",
    "    if database=='manga' and ranking_type=='bypopularity':\n",
    "        number_entries = 62_697\n",
    "    \n",
    "    if database=='manga' and ranking_type=='favorite':\n",
    "        number_entries = 70_303\n",
    "\n",
    "    last_page = math.ceil(number_entries / 500) - 1\n",
    "\n",
    "    # Test that it's still correct\n",
    "    params = {'ranking_type': ranking_type, 'limit': 500, 'offset': last_page*500}\n",
    "    data = get_data(f'/{database}/ranking', params)\n",
    "    assert len(data['data']) > 0\n",
    "    assert 'next' not in data['paging']\n",
    "\n",
    "    return last_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_keys = [\n",
    "    'id', 'title', 'main_picture', 'alternative_titles', 'start_date', 'end_date', 'synopsis', 'mean', 'rank', 'popularity',\n",
    "    'num_list_users', 'num_scoring_users', 'num_favorites', 'nsfw', 'genres', 'created_at', 'updated_at', 'media_type', 'status'\n",
    "]\n",
    "\n",
    "anime_keys = [*common_keys, 'num_episodes', 'start_season', 'broadcast', 'source', 'average_episode_duration', 'rating', 'studios']\n",
    "\n",
    "manga_keys = [*common_keys, 'num_volumes', 'num_chapters', 'authors{id,first_name,last_name}']\n",
    "\n",
    "keys = {'anime': anime_keys, 'manga': manga_keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def merge_anime(tmp_directory, save_file_path):\n",
    "\n",
    "    data = []\n",
    "    for file_name in os.listdir(tmp_directory):\n",
    "        file_path = os.path.join(tmp_directory, file_name)\n",
    "        with open(file_path, 'r') as f:\n",
    "            file = json.load(f)\n",
    "        data.extend(file)\n",
    "\n",
    "    with open(save_file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    #shutil.rmtree(tmp_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manga Alternative Titles Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_fails = [116770, 144472, 115838, 143751, 146583, 148716]\n",
    "\n",
    "def manga_crash(endpoint, params):\n",
    "    page = params[\"offset\"]//params[\"limit\"]\n",
    "    print(f'Crashed at page {page}')\n",
    "    \n",
    "    params['fields'] = params['fields'].replace('alternative_titles,', '')\n",
    "    data = get_data(endpoint, params)\n",
    "\n",
    "    ids = [manga['node']['id'] for manga in data['data']]\n",
    "\n",
    "    present_fails = [id for id in ids if id in known_fails]\n",
    "\n",
    "    if not present_fails:\n",
    "        print('Fail unknown...')\n",
    "        return data\n",
    "    \n",
    "    print('Fails:', present_fails)\n",
    "\n",
    "    offset = page * params['limit']\n",
    "    problems = [offset-1]\n",
    "    for fail in known_fails:\n",
    "        if fail in ids:\n",
    "            problems.append(offset + ids.index(fail))\n",
    "    problems.append(offset + params['limit'])\n",
    "\n",
    "    alternative_titles = []\n",
    "    params['fields'] = 'alternative_titles'\n",
    "    for i in range(len(problems)-1):\n",
    "        params['offset'] = problems[i] + 1\n",
    "        params['limit'] = problems[i+1] - problems[i] - 1\n",
    "        data_short = get_data(endpoint, params)\n",
    "        alternative_titles.extend((manga['node']['id'], manga['node']['alternative_titles']) for manga in data_short['data'])\n",
    "        time.sleep(1)\n",
    "\n",
    "    for id, alt_tit in alternative_titles:\n",
    "        data['data'][ids.index(id)]['node']['alternative_titles'] = alt_tit\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped at: 2022-07-23 20.55.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [02:26<00:00,  2.99s/it]\n"
     ]
    }
   ],
   "source": [
    "scrape_ranking('anime', 'favorite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped at: 2022-07-23 20.59.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [08:09<00:00,  4.08s/it]\n"
     ]
    }
   ],
   "source": [
    "scrape_ranking('manga', 'bypopularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped at: 2022-07-25 10.57.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 66/135 [04:27<04:42,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crashed at page 66\n",
      "Fails: [116770]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 83/135 [05:39<03:37,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crashed at page 83\n",
      "Fails: [144472]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 94/135 [06:31<02:58,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crashed at page 94\n",
      "Fails: [115838]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 97/135 [06:47<03:02,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crashed at page 97\n",
      "Fails: [143751]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 102/135 [07:13<02:34,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crashed at page 102\n",
      "Fails: [146583]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 128/135 [09:07<00:29,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crashed at page 128\n",
      "Fail unknown...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [09:33<00:00,  4.25s/it]\n"
     ]
    }
   ],
   "source": [
    "scrape_ranking('manga', 'favorite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lost 499 alternative_titles at page 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2117d8da04c720903da65ba8862d63b2fb82849cd17efbeca69c80f28b43db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
